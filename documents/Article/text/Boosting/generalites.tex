\subsection{Qu'est-ce que le boosting?}
Le boosting est une méthode de Machine Learning apparue à la fin des années 1980 et ayant évoluée au fil du temps en plusieurs version, les principales étant AdaBoost ou encore les GBM (\textit{Gradient Boosted Models})\mysidenote{{\large\thColor{Historique du Boosting}} \\ \textbf{1989} \\ Proposition de la méthode et premier algorithme par R. \textsc{Schapire} \\ \textbf{1996} \\ Première implémentation d'AdaBoost par Y. \textsc{Freund} et R. \textsc{Schapire} \\ \textbf{1999} \\ Apparition du Boosting de gradient (GBM) par L. \textsc{Breiman} et J. \textsc{Freidman} \\ \textbf{2014} \\ Apparition et implémentation de XGBoost par T. \textsc{Chen}}. 

Le succès de ces méthodes provient de leur manière originale d'utiliser des algorithmes déjà existant au sein d'une stratégie adaptative. Cette stratégie leur permet alors de convertir un ensemble de règles et modèles peu performant en les combinant pour obtenir de (très) bonnes prédictions. L'idée principale est en effet d'ajouter de nouveaux modèles au fur et à mesure, mais de réaliser ces ajouts en accord avec un critère donné. En ce sens, cette famille de méthodes se différencie des Random Forest qui vont elles miser sur l'aléatoire pour moyenner l'erreur.

Cet aspect fondamental du boosting permet ainsi une forte réduction du biais et de la variance de l'estimation, mais surtout garanti une convergence rapide. La contrepartie se fait au niveau de la sensibilité au bruit comme nous le verrons dans la description des algorithmes.